# Differential-Privacy
I will compare model accuracy of original and synthetic dataset which generated by Differential Privacy.
Large	datasets	are	becoming	more	widely	available	in	a	range	of	areas.	In	the	United	
States,	for	example	[1],	the	percentage	of	doctors	who	use	electronic	health	records	
(EHR)	has	risen	from	9.4	percent	in	2008	to	83.8	percent	in	2015.	The	availability	
of	massive	datasets	opens	up	a	plethora	of	possibilities	for	collaboration	between	
data	owners	and	the	machine	learning	community.	However,	many	of	these	massive	
datasets,	particularly	EHRs,	contain	sensitive	data	 that	inhibits	data	owners	 from	
sharing	the	information.	The	most	common	technique	to	reduce	the	privacy	risk	of	
sharing	 sensitive	 records	 is	 to	 de-identify	 them.	 However,	 it	 is	 now	 generally	
understood	that	de-identified	records	can	be	easily	re-identified	by	linking	them	to	
other	identifiable	datasets.	Nevertheless,	if	the	goal	of	sharing	the	data	is	to	create	
and verify	machine	learning	methods	for	a	specific	task	(for	example,	prognostic	risk	
score),	real	data	isnâ€™t	required;	synthetic	data	that	is	sufficiently	similar	to	real	data	
would	suffice. 


In	 this	 study,	 we	 present	 a	 differentially	 private	 privacy	 preserving	 generative	
adversarial	network	(DPGAN)	that	maintains	the	privacy	of	the	training	data.	Our	
technique	 has	 been	 convincingly	 demonstrated	 to	 ensure	
(epsilon,gamma)differential	privacy.	We ran	four	tests	to	show	that	our	algorithm	
can	 create	 highquality	 data	 points	 and	 converges	 under	 both	 noisy	 and	 limited	
training	data	conditions,	with	usable	learning	curves	for	tuning	hyperparameters.	I	
use	classification	methods	while	comparing	the	original	and	produced	datasets.	The	
major	goal	of	 this	experiment	was	 to	demonstrate	 the	impact	of	various	levels	of	
noise	on	model	accuracy.	As	noise	is	introduced,	the	accuracy	of	the	classifier	built	
on	generated	data	improves,	indicating	that	the	generated data	is	of	greater	quality.
In	 the	 first	 three	 dataset	 (NHANES,	 Credit	 and	 Adult)	 example,	 we	 observe	
maximum	model	accuracy	for	generated	dataset	is	around	50	%.		However,	in	the	
Churn	dataset,	we	get	more	similar	accuracy	result	to	original	one	which	is 80	%.	
Additionally,	 once	 the	 epsilon	 value	 reaches	 approximately	 0.25,	 the	 correlation	
coefficient	 of	 the	 Churn	 and	 NHANES datasets	 begins	 to	 increase. The	 produced	
dataset	had	a	maximum	accuracy	 value	in	 the	0.25-0.5	epsilon	interval	across	all	
datasets. Another	insight	is	that	classification	accuracy	depends	on	data	types	in	the	
original	 and	 generated	 dataset.	 Adult	 and	 Credit	 dataset	 has	 similar	 number	 of	
attributes	 	 which	 is	 14.	 However	 classification	 accuracy	 result	 on	 their	 original	
dataset	is	not	the	same.	So,	we	observe	68%	in	the	credit	dataset	and	81	percent	in	
the	adult	data.			From	 the	point	of	row	numbers,	Churn	and	NHANES	dataset	has	
around	6000	rows.	Their	accuracy	for	original	and	generated	dataset	are	the	same.		
For	example,	generated	dataset	shows	maximum	accuracy	level	in	the	0.25	epsilon	
level.		So,	we	can	say	that	row	numbers	are	more	important	than	attributes.		In	future	
work,	 we	 recommended	 to	 try	 different	 privacy	 algorithms	 for	 learning	 such	
relationship	 in	 better	 way.	 Additionally,	 epsilon	 value	 can	 be	 in	 1-10	 range	 for	
further	research.
